{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nested image classification\n",
    "## Data processing part 2\n",
    "\n",
    "This is a notebook documenting the second part of the data processing on the image dataset provided by nested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import libraries and nested_utilities\n",
    "import nested_utilities as nutil\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#\n",
    "# create a timestamped directory for this processing path\n",
    "out_dir = os.path.join('./data_catalogues/processing_'+nutil.timestamp())\n",
    "os.mkdir(out_dir)\n",
    "#out_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. SPLITTING TEST & TRAIN DATA\n",
    "\n",
    "Split the data into a train set and test set.\n",
    "\n",
    "First of all remove any existing train / test split directories.  \n",
    "Then copy the `base_data/` directories and files into `splitting_data/`.   \n",
    "Remove `uncertain/` and `exclude/`.   \n",
    "Create directories `test_data/` and `model_data/` for the test and train datasets.   \n",
    "The use a random number generator to move a % of images to `test_data/` in desired proportion (default: 0.25 test data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove existing dirs if exist\n",
    "#shutil.rmtree('./data/model_data/')\n",
    "#shutil.rmtree('./data/test_data/')\n",
    "#shutil.rmtree('./data/splitting_data/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/splitting_data/'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# copy base_data to 'splitting_data/'\n",
    "shutil.copytree('./data/base_data/', './data/splitting_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete 'exclude' and 'uncertain' directories from '/splitting_data/'\n",
    "shutil.rmtree('./data/splitting_data/exclude/')\n",
    "shutil.rmtree('./data/splitting_data/uncertain/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test_data and model_data directories\n",
    "nutil.create_test_dirs('./data/splitting_data/', './data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14922 files found: 3740 allocated to test and 11182 to modelling\n"
     ]
    }
   ],
   "source": [
    "# split the data to model_data and test_data\n",
    "util.data_model_test_split('./data/splitting_data/', './data/test_data/',\n",
    "                           test_split=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Creating training and validation directories, and splitting model_data between them\n",
    "\n",
    "Takes remaining (non-test) data in `splitting_data/` and moves to `model-data`, splitting (again) between `model_data/training_dir/` and `model_data/validation_dir` in defined proportions based on a random number generator.   \n",
    "(default proportions: 0.7 train, 0.3 validation).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutil.create_train_and_val_dirs('./data/splitting_data/','./data/model_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11182 files copied: 7840 training and 3342 validation\n"
     ]
    }
   ],
   "source": [
    "nutil.data_train_val_split('./data/splitting_data/','./data/model_data/',\n",
    "                          val_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove splitting dir\n",
    "shutil.rmtree('./data/splitting_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
